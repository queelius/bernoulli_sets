%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Appendix
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\section{Additional proofs}
%In what follows, we show additional or more rigorous proofs that are not necessarily central to the paper but warrant mention.

\section{Proof of corollary~\ref{cor:fpr_as_vareps}}
\label{app:cor_fpr_as_vareps}
To say that the sequence $\alpha_1,\alpha_2,\ldots$ converges almost surely to $\fprate$ means that
\begin{equation}
\Prob{\lim _{n \to \infty} \alpha_n = \fprate} = 1.
\end{equation}

By \cref{cor:fpr_as_vareps}, given \emph{countably infinite} negatives, a random approximate set with a false positive rate $\fprate$ is \emph{certain} to obtain $\fprate$.
\begin{proof}
Hoeffding's inequality~\cite{hoeffding} provides that $\FP_n$ is concentrated around its mean $n \fprate$ as given by
\begin{equation}
\Prob{(\fprate -\epsilon) n \leq \FP_n \leq (\fprate + \epsilon )n}
\geq 1 - 2 \exp\!\left(-2\epsilon ^2 n\right),
\end{equation}
where $\epsilon > 0$.
For any $\epsilon > 0$, Hoeffding's inequality gives $\Prob{|\alpha_n - \fprate| > \epsilon} \leq 2\exp(-2\epsilon^2 n)$.
Since $\sum_{n=1}^{\infty} 2\exp(-2\epsilon^2 n) < \infty$ (geometric series), the Borel--Cantelli lemma yields $\Prob{|\alpha_n - \fprate| > \epsilon \text{ i.o.}} = 0$.
As this holds for every $\epsilon > 0$, $\alpha_n \to \fprate$ almost surely.
\end{proof}

\section{Proof of theorem~\ref{thm:approx_expected_precision}}
\label{sec:proof_approx_expected_precision}
Given $p$ positives and $n$ negatives, by \cref{thm:approx_expected_precision} an approximate set with a false positive rate $\fprate$ and a false negative rate $\fnrate$ has an \emph{expected} precision given \emph{approximately} by
\begin{equation*}
    \ppv(\fnrate, \fprate ; n, p) \approx \frac{\overline{t}}{\overline{t} + \overline{f}} +
    \frac{\overline{t} \sigma_{\!f}^2 - \overline{f}_p \sigma_{\!t}^2}{\left(\overline{t} + \overline{f}\right)^3},
    \tag{\ref{eq:approx_expected_precision} revisited}
\end{equation*}
where $\overline{t} = p \tprate$ is the \emph{expected} number of \emph{true positives}, $\overline{f} =  n \fprate$ is the \emph{expected} number of \emph{false positives}, $\sigma_{\!t}^2 = p \fnrate \tprate$ is the variance of the number of \emph{true positives}, and $\sigma_{\!f}^2 = n \fprate \fnrate$ is the variance of the number of \emph{false positives}.
\begin{proof}
The positive predictive value is a random variable given by
\begin{equation}
    \frac{\TP_p}{\TP_p + \FP_n}.
\end{equation}
We are interested in the \emph{expected} positive predictive value,
\begin{equation}
    \ppv(\fprate,\tprate) = \Expect{\frac{\TP_p}{\TP_p + \FP_n}}.
\end{equation}
This expectation is of a non-linear function of random variables, which is problematic so we choose to approximate the expectation.

Let the \emph{positive predictive value} function be denoted by
\begin{equation}
    f(t, f) = \frac{t}{t + f},
\end{equation}
where $t$ is the number of true positives and $f$ is the number of false positives.
We approximate this function with a second-order Taylor series.
The gradient of $f$ is given by
\begin{equation}
    \nabla f(t,f) =
    \frac{1}{(t + f)^2}
    \begin{bmatrix}
        f\\
        -t\\
    \end{bmatrix}
\end{equation}
and the Hessian of $f$ is given by
\begin{equation}
    \mathcal{H}(t,f) =
    \frac{1}{(t + f)^3}
    \begin{bmatrix}
        -2 f & t-f\\
        t-f & 2 t \\
    \end{bmatrix}
\end{equation}

A linear approximation $\Fun{g}$ of $f$ that is reasonably accurate near the expected value of $\TP_p$, denoted by $\overline{t}$, and the expected value of $\FP_n$, denoted by $\overline{f}$, is given by
\begin{equation}
    \Fun{g}(t,f) =
    f\left(\overline{t},\overline{f}\right) + \nabla f(\overline{t},\overline{f})^{\intercal}
    \begin{bmatrix}
        t - \overline{t}\\
        f - \overline{f}\\
    \end{bmatrix}
    + \frac{1}{2}
    \begin{bmatrix}
        t - \overline{t}\\
        f - \overline{f}\\
    \end{bmatrix}^{\intercal}
    \mathcal{H}(\overline{t},\overline{f})
    \begin{bmatrix}
        t - \overline{t}\\
        f - \overline{f}\\
    \end{bmatrix}
\end{equation}
As a function of random variables $\TP_p$ and $\FP_n$, $\Fun{g}\!\left(\TP_p,\FP_n\right)$ is a random variable.
Since $\Expect{\TP_p - \overline{t}} = 0$ and $\Expect{\FP_n - \overline{f}} = 0$, we immediately simplify the expectation of $\Fun{g}$ to
\begin{equation}
\label{eq:proof_hess1}
    \Expect{\Fun{g}(\TP_p,\FP_n)} = \frac{\overline{t}}{\overline{t}+\overline{f}} + \frac{\Expect{A}}{(\overline{t} + \overline{f})^3}
\end{equation}
where
\begin{equation}
    A = \frac{1}{2}
    \begin{bmatrix}
        \TP_p - \overline{t}\\
        \FP_n - \overline{f}
    \end{bmatrix}^{\intercal}
    \begin{bmatrix}
        -2 \overline{f} \left(\TP_p - \overline{t}\right) + \left(\overline{t}-\overline{f}\right)\left(\FP_n - \overline{f}\right)\\
        \left(\overline{t}-\overline{f}\right)\left(\TP_p - \overline{t}\right) + 2 \overline{t}\left(\FP_n - \overline{f}\right)
    \end{bmatrix}
\end{equation}
Multiplying the right column matrix by the Hessian matrix in $A$ results in
\begin{equation}
    A = \frac{1}{2}
    \begin{bmatrix}
        \TP_p - \overline{t}\\
        \FP_n - \overline{f}
    \end{bmatrix}^{\intercal}
    \begin{bmatrix}
        -2 \overline{f} \left(\TP_p - \overline{t}\right) + \left(\overline{t}-\overline{f}\right)\left(\FP_n - \overline{f}\right)\\
        \left(\overline{t}-\overline{f}\right)\left(\TP_p - \overline{t}\right) + 2 \overline{t}\left(\FP_n - \overline{f}\right)
    \end{bmatrix}
\end{equation}
Multiplying the left column matrix by the right column matrix in $A$ results in
\begin{equation}
    A = -\overline{f} \left(\TP_p - \overline{t}\right)^2 + 
    \left(\overline{t}-\overline{f}\right)\left(\TP_p - 
    \overline{t}\right)\left(\FP_n - \overline{f}\right) + 
    \overline{t}\left(\FP_n - \overline{f}\right)^2
\end{equation}
As a linear operator, the expectation of $A$ is equivalent to
\begin{equation}
    \Expect{A} = -\overline{f} \Expect{\TP_p - \overline{t}}^2 + \left(\overline{t}-\overline{f}\right)\Expect{\left(\FP_n - \overline{f}\right)\left(\TP_p - \overline{t}\right)} + \overline{t}\Expect{\FP_n - \overline{f}}^2
\end{equation}
By definition, $\Expect{\TP_p - \overline{t}}^2$ is the variance of $\TP_p$, $\Expect{\FP_n - \overline{f}}^2$ is the variance of $\FP_n$, and $\Expect{\left(\FP_n - \overline{f}\right)\left(\TP_p - \overline{t}\right)}$ is the covariance of $\TP_p$ and $\FP_n$, which is $0$ since they are independent.
Making these substitutions results in
\begin{equation}
    \Expect{A} = \overline{t} \Var{\FP_n} - \overline{f} \Var{\TP_p}
\end{equation}
Substituting this result into \cref{eq:proof_hess1} yields
\begin{equation}
    \Expect{\Fun{g}(\TP_p,\FP_n)} = 
    \frac{\overline{t}}{\overline{t}+\overline{f}} + 
    \frac{-\overline{f} \Var{\TP_p} + 
    \overline{t}\Var{\FP_n}}{(\overline{t} + \overline{f})^3}
\end{equation}
By \cref{thm:fpbinom}, $\FP_n$ is binomially distributed with a mean $n \fprate$ and a variance $n \fprate \tnrate$ and by \cref{cor:tpbinom}, $\TP_p$ is binomially distributed with a mean $p \tprate$ and a variance $p \fnrate \tprate$.
Substituting $\Var{\FP_n} = n \fprate(1-\fprate) = \overline{f}_p(1-\fprate) = \sigma_{f_p}^2$ and $\Var{\TP_p} = p \tprate(1-\tprate) = \overline{t}_p(1-\tprate) = \sigma_{t_p}^2$ completes the proof.
\end{proof}

\section{Sampling distribution of arbitrary functions}
\label{app:samp}
Suppose we have a function $f : \PS{\Set{X}[1]} \times \cdots \times \PS{\Set{X}[q]} \to \Set{Y}$, and we are interested in evaluating the loss when we replace one or more of the objective input sets with particular corresponding random approximate sets.
The result of this substitution, as previously described, is a probability distribution over $\Set{Y}$.

The probability distribution of random approximate sets are precisely given; therefore, we may estimate the distribution of any function of random approximate sets by generating the random approximate sets and applying the function of interest.

Consider the $m$-by-$q$ matrix where the $(i,j)$-th element is the random approximate set $\tilde{A}_{i\,j}$ such that each random element of the matrix is independently and each column is identically distributed.
If we apply $\Fun{g}$ to each row of the matrix,
\begin{equation}
\RV{Y}_i = \Fun{g}\left(\tilde{A}_{i\,1},\ldots,\tilde{A}_{i\,q}\right)
\end{equation}
for $i=1,\ldots,m$, we generate $m$ i.i.d. random elements $\RV{Y_1},\ldots,\RV{Y_m}$.

If $\Set{Y}$ is a measure space (discrete or continuous), consider the random variable
\begin{equation}
\RV{\overline{Y}_m} = \frac{1}{m} \sum_{i=1}^m \RV{Y_i}
\end{equation}
If $\RV{Y_1}$ has a well-defined mean and variance, then by the central limit theorem
\begin{equation}
\lim_{m \to \infty} \RV{\overline{Y}_m}
\end{equation}
converges in distribution to a normal with a mean $\Expect{\RV{Y_1}}$ and a variance $\Var{\RV{Y_1}} / m$

A general approach to estimating $\RV{\overline{Y}_m}$ is given by generating a large sample of matrices and applying the function $\Fun{g}$ to each to generate a large sample from $\RV{Y_1}$

% Add this before the bibliography

\section{Notation Reference}

\subsection{Basic Set Notation}
\begin{itemize}
  \item $\Set{S}$, $\Set{A}$, $\Set{B}$ --- Standard notation for sets
  \item $\EmptySet$ --- The empty set
  \item $|\Set{A}|$ --- The cardinality of set $\Set{A}$
  \item $\Set{A} \times \Set{B}$ --- The Cartesian product of sets $\Set{A}$ and $\Set{B}$, defined as $\SetBuilder{\Pair{a}{b}}{a \in \Set{A} \land b \in \Set{B}}$
  \item $\Set{B}^n$ --- The $n$-fold Cartesian product $\Set{B} \times \cdots \times \Set{B}$ ($n$ times)
  \item $2^{\Set{A}}$ --- The power set of $\Set{A}$
  \item $\SetIndicator{\Set{A}}$ --- The indicator function mapping elements to $\{0,1\}$, where $\SetIndicator{\Set{A}}(x) = 1$ if $x \in \Set{A}$ and $0$ otherwise
  \item $\Set{A} \cup \Set{B}$ --- The union of sets $\Set{A}$ and $\Set{B}$
  \item $\Set{A} \cap \Set{B}$ --- The intersection of sets $\Set{A}$ and $\Set{B}$
  \item $\Set{A} \setminus \Set{B}$ --- The set difference of $\Set{A}$ and $\Set{B}$
  \item $\Set{A}^C$ --- The complement of set $\Set{A}$
\end{itemize}

\subsection{Common Sets}
\begin{itemize}
  \item $\RealSet$ --- The set of real numbers $(-\infty,\infty)$
  \item $\NatSet$ --- The set of natural numbers
  \item $\BitSet$ --- The binary set $\{0,1\}$
  \item $\IntSet$ --- The set of integers
  \item $\RatSet$ --- The set of rational numbers
\end{itemize}

\subsection{Approximate Sets}
\begin{itemize}
  \item $\tilde{A}$ --- A random approximate set of $\Set{A}$ with unspecified parameters
  \item $\tilde{A}^\fprate_\tprate$ --- A random approximate set of $\Set{A}$ with false positive rate $\fprate$ and true positive rate $\tprate$
  \item $\tilde{A}[\fprate][-]$ --- A random approximate set of $\Set{A}$ with false positive rate $\fprate$ and unspecified true positive rate
  \item $\tilde{A}[+][\tprate]$ --- A random approximate set of $\Set{A}$ with unspecified false positive rate and true positive rate $\tprate$
  \item $\tilde{A}_+$ --- A positive random approximate set of $\Set{A}$ (no false negatives)
  \item $\tilde{A}_+^\fprate$ --- A positive random approximate set of $\Set{A}$ with false positive rate $\fprate$
  \item $\tilde{A}_-$ --- A negative random approximate set of $\Set{A}$ (no false positives)
  \item $\tilde{A}_-^\fnrate$ --- A negative random approximate set of $\Set{A}$ with false negative rate $\fnrate$
\end{itemize}

\subsection{Error Rates}
\begin{itemize}
  \item $\fprate$ --- Expected false positive rate
  \item $\fnrate$ --- Expected false negative rate
  \item $\tprate$ --- Expected true positive rate, where $\tprate = 1 - \fnrate$
  \item $\tnrate$ --- Expected true negative rate, where $\tnrate = 1 - \fprate$
  \item $\fprateob$ --- Observed false positive rate
  \item $\fnrateob$ --- Observed false negative rate
  \item $\tprateob$ --- Observed true positive rate
  \item $\tnrateob$ --- Observed true negative rate
\end{itemize}

\subsection{Random Variables and Probability}
\begin{itemize}
  \item $\RV{X}$ --- A random variable (denoted in mathrm font)
  \item $f_{\RV{X}}$ --- The probability mass/density function of random variable $\RV{X}$
  \item $F_{\RV{X}}$ --- The cumulative distribution function of random variable $\RV{X}$
  \item $\Prob{\cdot}$ --- The probability measure
  \item $E\{\cdot\}$ --- The expectation operator
  \item $\Var{\cdot}$ --- The variance operator
\end{itemize}

\subsection{Efficiency Measures}
\begin{itemize}
  \item $\BL$ --- Bit length function measuring storage requirements
  \item $\RE$ --- Relative efficiency function comparing data structures
  \item $\AbsoluteEfficiency$ --- Absolute efficiency function comparing to theoretical lower bounds
\end{itemize}

\subsection{Binary Classification Measures}
\begin{itemize}
  \item $\TP$, $\FP$, $\TN$, $\FN$ --- True positives, false positives, true negatives, and false negatives
  \item $\PPV$ --- Positive predictive value (precision)
  \item $\NPV$ --- Negative predictive value
  \item $\FDR$ --- False discovery rate
  \item $\ACC$ --- Accuracy measure
  \item $\FOR$ --- False omission rate
\end{itemize}

\subsection{Set-Theoretic Operations (Section~\ref{sec:set_theory})}
\begin{itemize}
  \item $w_1, w_2, w_3$ --- Partition weights used in union/intersection error rate derivations; see equations (5.3), (5.7), and (5.11) for their precise definitions
  \item $\fprate_1, \fprate_2$ --- False positive rates of two input approximate sets
  \item $\fnrate_1, \fnrate_2$ --- False negative rates of two input approximate sets
\end{itemize}

\subsection{Entropy (Section~\ref{sec:entropy})}
\begin{itemize}
  \item $\Entropy{\cdot}$ --- Shannon entropy
  \item $\Entropy{\cdot \Given \cdot}$ --- Conditional entropy
  \item $\FP_m$ --- Random number of false positives given $m$ positives
  \item $\FN_m$ --- Random number of false negatives given $m$ positives
  \item $\POS$ --- Random variable for the number of positives
  \item $\NEG$ --- Random variable for the number of negatives, $\NEG = u - \POS$
\end{itemize}

\subsection{Boolean Search (Section~\ref{sec:bool_search})}
\begin{itemize}
  \item $\Set{K}$ --- Set of search keys
  \item $\Set{D}$ --- Set of documents
  \item $Q$ --- Boolean algebra of queries over $\PS{\Set{K}}$
  \item $\operatorname{facts}(d)$ --- Search index mapping document $d$ to a subset of $\Set{K}$
  \item $\operatorname{find}(q)$ --- Boolean algebra homomorphism mapping query $q$ to its result set
  \item $\operatorname{find}^\sigma(q)$ --- Approximate find function operating on approximate set indexes
  \item $\operatorname{encrypted\_find}(q)$ --- Encrypted find composition $\operatorname{find}^\sigma \circ H$
  \item $\operatorname{h} \colon \BitSet^* \to \BitSet^k$ --- Cryptographic hash function mapping keys to trapdoors
\end{itemize}
